{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_drive_loc = r'C:\\Users\\uscd675041\\WSP O365\\chattanooga-modeling - RTP TDM Update'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Data Needed\n",
    "- TPO Boundary for verifying II, IE, EI, and EE patterns\n",
    "- INRIX Data\n",
    "- Geoprocess the Start and End Points\n",
    "\n",
    "<b>Only Needed If Running from Scratch</b>  \n",
    "Otherwise, use the Parquet table in the next cell."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Read TPO Boundary...')\n",
    "tpo = gpd.read_file(r'C:\\Users\\uscd675041\\WSP O365\\chattanooga-modeling - RTP TDM Update\\Data\\tpo_boundary.shp').to_crs(epsg=4326)\n",
    "tpo_grid = gpd.read_file(r'C:\\Users\\uscd675041\\WSP O365\\chattanooga-modeling - RTP TDM Update\\Data\\tpo_boundary_grid_4326.shp')\n",
    "districts = gpd.read_file(r'C:\\Users\\uscd675041\\WSP O365\\chattanooga-modeling - RTP TDM Update\\Data\\Districts\\districts.shp').to_crs(epsg=4326)\n",
    "minx,miny,maxx,maxy = tpo.bounds.iloc[0]\n",
    "\n",
    "cols = {\n",
    "    'TRIP_ID': {'position': 0, 'col_type': str},\n",
    "    'DEVICE_ID': {'position': 1, 'col_type': str},\n",
    "    'PROVIDER_ID': {'position': 2, 'col_type': str},\n",
    "    'MODE': {'position': 3, 'col_type': np.int8},\n",
    "    'START_DATE': {'position': 4, 'col_type': str},\n",
    "    'END_DATE': {'position': 6, 'col_type': str},\n",
    "    'START_LAT': {'position': 8, 'col_type': np.float},\n",
    "    'START_LON': {'position': 9, 'col_type': np.float},\n",
    "    'END_LAT': {'position': 10, 'col_type': np.float},\n",
    "    'END_LON': {'position': 11, 'col_type': np.float},\n",
    "    'GEO_TYPE': {'position': 12, 'col_type': str},\n",
    "    'PROVIDER_TYPE': {'position': 13, 'col_type': np.int8},\n",
    "    'PROFILE': {'position': 14, 'col_type': np.int8},\n",
    "    'VEH_CLASS': {'position': 15, 'col_type': np.int8}\n",
    "}\n",
    "\n",
    "cwd = r'C:\\Users\\uscd675041\\WSP O365\\chattanooga-modeling - RTP TDM Update\\Data\\INRIX\\{}\\data'\n",
    "\n",
    "months = ['May_2019', 'October_2019']\n",
    "trip_files = ['trips.csv', 'trips.csv1']\n",
    "\n",
    "trips = []\n",
    "\n",
    "\n",
    "for month in months:\n",
    "    for trip_file in trip_files:\n",
    "        print('Reading: {}'.format(os.path.join(cwd.format(month), trip_file)))\n",
    "        trips.append(pd.read_csv(os.path.join(cwd.format(month), trip_file), header=None,\n",
    "                                 usecols=[v['position'] for v in cols.values()],\n",
    "                                 dtype={meta['position']: meta['col_type'] for meta in cols.values()},\n",
    "                                 names=cols.keys()))\n",
    "\n",
    "\n",
    "print('Concatenating...')\n",
    "trips = pd.concat(trips)\n",
    "\n",
    "\n",
    "\n",
    "print('Generating Geometries...')\n",
    "trip_starts = gpd.GeoDataFrame(trips[['TRIP_ID', 'START_LON', 'START_LAT']], \n",
    "                               geometry=[Point(xy) for xy in zip(trips['START_LON'], trips['START_LAT'])],\n",
    "                       crs='epsg:4326'\n",
    "                      )\n",
    "trip_ends = gpd.GeoDataFrame(trips[['TRIP_ID', 'END_LON', 'END_LAT']], \n",
    "                             geometry=[Point(xy) for xy in zip(trips['END_LON'], trips['END_LAT'])],\n",
    "                             crs='epsg:4326'\n",
    "                            )\n",
    "\n",
    "trip_starts['GEO_TYPE'] = np.nan\n",
    "trip_ends['GEO_TYPE'] = np.nan\n",
    "\n",
    "trip_starts.loc[~((trip_starts['START_LON'].between(minx, maxx)) & (trip_starts['START_LAT'].between(miny, maxy))), 'GEO_TYPE'] = 'E'\n",
    "trip_ends.loc[~((trip_ends['END_LON'].between(minx, maxx)) & (trip_ends['END_LAT'].between(miny, maxy))), 'GEO_TYPE'] = 'E'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = pd.read_parquet(os.path.join(one_drive_loc, 'Data', 'INRIX', 'INRIX_Merged_Trips.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_personal = trips[(trips['START_DATE'].dt.month == 5) & trips['START_DATE'].dt.dayofweek.isin([1,2,3]) &\n",
    "                     (trips['MODE'] == 1) & (trips['VEH_CLASS'] == 1) & (trips['PROFILE'] == 1)\n",
    "                    ]\n",
    "\n",
    "oct_personal = trips[(trips['START_DATE'].dt.month == 10) & trips['START_DATE'].dt.dayofweek.isin([1,2,3]) &\n",
    "                     (trips['MODE'] == 1) & (trips['VEH_CLASS'] == 1) & (trips['PROFILE'] == 1)\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_personal_mtx_abs = may_personal.groupby(['START_DISTRICT', 'END_DISTRICT'])[['GEO_TYPE']].count().rename(columns={'GEO_TYPE': 'TRIPS'}).unstack(-1).fillna(0).astype(int)\n",
    "may_personal_mtx_abs.to_csv(os.path.join(one_drive_loc, 'Data', 'INRIX', 'may_personal_auto_abs.csv'))\n",
    "may_personal_mtx_pct = may_personal_mtx_abs.drop(columns=('TRIPS', -1)).loc[1:] / may_personal_mtx_abs.drop(columns=('TRIPS', -1)).loc[1:].sum().sum()\n",
    "may_personal_mtx_pct.to_csv(os.path.join(one_drive_loc, 'Data', 'INRIX', may_personal_auto_pct.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oct_personal_mtx_abs = oct_personal.groupby(['START_DISTRICT', 'END_DISTRICT'])[['GEO_TYPE']].count().rename(columns={'GEO_TYPE': 'TRIPS'}).unstack(-1).fillna(0).astype(int)\n",
    "oct_personal_mtx_abs.to_csv(os.path.join(one_drive_loc, 'Data', 'INRIX', 'oct_personal_auto_abs.csv'))\n",
    "oct_personal_mtx_pct = oct_personal_mtx_abs.drop(columns=('TRIPS', -1)).loc[1:] / oct_personal_mtx_abs.drop(columns=('TRIPS', -1)).loc[1:].sum().sum()\n",
    "oct_personal_mtx_pct.to_csv(os.path.join(one_drive_loc, 'Data', 'INRIX', 'oct_personal_auto_pct.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_truck = trips[(trips['START_DATE'].dt.month == 5) & trips['START_DATE'].dt.dayofweek.isin([1,2,3]) &\n",
    "                     (trips['VEH_CLASS'] != 1)\n",
    "                    ]\n",
    "\n",
    "oct_truck = trips[(trips['START_DATE'].dt.month == 10) & trips['START_DATE'].dt.dayofweek.isin([1,2,3]) &\n",
    "                     (trips['VEH_CLASS'] != 1)\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_truck_mtx_abs = may_truck.groupby(['START_DISTRICT', 'END_DISTRICT'])[['GEO_TYPE']].count().rename(columns={'GEO_TYPE': 'TRIPS'}).unstack(-1).fillna(0).astype(int)\n",
    "may_truck_mtx_abs.to_csv(os.path.join(one_drive_loc, 'Data', 'INRIX', 'may_truck_abs.csv'))\n",
    "may_truck_mtx_pct = may_truck_mtx_abs.drop(columns=('TRIPS', -1)).loc[1:] / may_truck_mtx_abs.drop(columns=('TRIPS', -1)).loc[1:].sum().sum()\n",
    "may_truck_mtx_pct.to_csv(os.path.join(one_drive_loc, 'Data', 'INRIX', 'may_truck_pct.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oct_truck_mtx_abs = oct_truck.groupby(['START_DISTRICT', 'END_DISTRICT'])[['GEO_TYPE']].count().rename(columns={'GEO_TYPE': 'TRIPS'}).unstack(-1).fillna(0).astype(int)\n",
    "oct_truck_mtx_abs.to_csv(os.path.join(one_drive_loc, 'Data', 'INRIX', 'oct_truck_abs.csv'))\n",
    "oct_truck_mtx_pct = oct_truck_mtx_abs.drop(columns=('TRIPS', -1)).loc[1:] / oct_truck_mtx_abs.drop(columns=('TRIPS', -1)).loc[1:].sum().sum()\n",
    "oct_truck_mtx_pct.to_csv(os.path.join(one_drive_loc, 'Data', 'INRIX', 'oct_truck_pct.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_ext = may_personal.groupby(['GEO_TYPE_START', 'GEO_TYPE_END'])['MODE'].count()\n",
    "oct_ext = oct_personal.groupby(['GEO_TYPE_START', 'GEO_TYPE_END'])['MODE'].count()\n",
    "\n",
    "(may_ext.unstack(-1) / may_ext.unstack(-1).sum().sum()).to_clipboard(sep='\\t')\n",
    "\n",
    "(oct_ext.unstack(-1) / oct_ext.unstack(-1).sum().sum()).to_clipboard(sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "may_ext = may_truck.groupby(['GEO_TYPE_START', 'GEO_TYPE_END'])['MODE'].count()\n",
    "oct_ext = oct_truck.groupby(['GEO_TYPE_START', 'GEO_TYPE_END'])['MODE'].count()\n",
    "\n",
    "(may_ext.unstack(-1) / may_ext.unstack(-1).sum().sum()).to_clipboard(sep='\\t')\n",
    "\n",
    "#(oct_ext.unstack(-1) / oct_ext.unstack(-1).sum().sum()).to_clipboard(sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflake",
   "language": "python",
   "name": "snowflake"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
